# -*- coding: utf-8 -*-
"""TP4 - Inteligência Computacional.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xmkGSwaW2Ov6RhY7_qfLn-s2G8FWf2Yy

#GERAÇÃO DE LETRAS DE RAP A PARTIR DE UMA REDE NEURAL

A produção musical é uma tarefa complexa e vem sendo cada vez mais aprimorada e amparada por tecnologias auxiliares. A composição de letras de músicas é um exercício lógico, além da expressividade e essência transcrita. A vertente underground, traz consigo a originalidade e um ponto de vista crítico sobre situações cotidianas e, geralmente, é marginalizado pelo vocabulário ou com temas sensíveis. Este estudo explora o treinamento de uma LSTM para geração automatizada de letras de RAP, investigando os desafios e as oportunidades associadas a essa abordagem inovadora. Para alimentar essa rede neural foram utilizadas letras de música do grupo Racionais MC's, como forma de homenagem ao símbolo marcante que o grupo representa para o RAP nacional.

# Abordagens anteriores

Foram utilizadas algumas abordagens diferentes para o desenvolvimento deste trabalho antes de chegar na versão final do protótipo apresentado mais abaixo. Basicamente, a intenção era aproveitar das arquiteturas prontas dos modelos já existentes para ter uma geração de texto mais refinada e robusta. Porém, a limitação de hardware e ambientes de execução trouxeram problemas para a utilização destes modelos. Isso fez com que não pudessem ser aproveitados da maneira esperado, fazendo-se necessário a troca de abordagem.

## Utilização do modelo BERTimbau

A utilização deste modelo tinha a finalidade de geração de novas letras de RAP a partir da predição de labels [MASK] para tal. Inicialmente surgiram esperanças positivas, pois o modelo conseguia fazer a predição de palavras que completavam certas sentenças das letras contidas na base de dados. Não continham um score muito alto, mas acreditava-se que com o ajuste fino para enviesar o modelo no universo Racionais MC's este score seria maior, possibilitando a geração de novas letras coesas com a temática. Porém, a utilização deste modelo foi desencorajada pelo baixo indíce de acuráci a e todas as outras problemáticas que foram supracitadas. Seguem algumas informações visuais sobre os resultados do modelo.

### Testes de predição
"""

from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained("rufimelo/Legal-BERTimbau-base")
model = AutoModelForMaskedLM.from_pretrained("rufimelo/Legal-BERTimbau-base")

pipe = pipeline('fill-mask', model=model, tokenizer=tokenizer)
pipe('Hoje eu sou ladrão, [MASK] 157')
#Acertou na 4 tentativa

pipe = pipeline('fill-mask', model=model, tokenizer=tokenizer)
pipe('A [MASK] cheia, clareia as ruas do Capão')
#Acertou de primeira, porém com score baixo ainda.

"""## Utilização do modelo GPorTuguese-2

A utilização deste modelo tinha a finalidade de geração de novas letras de RAP a partir do propósito do modelo, geração de texto longo. A ideia parecia atrativa e com grande potencial, porém a utilização deste modelo, além das problemáticas supracitadas, não foi tão efetiva. O texto gerado não fazia muita relação com o vies que era desejado (gerar novas letras com similaridade às composições da base de dados). Por já possuir um extenso corpus pré-treinado, crê-se que seria necessário também a inclusão de um corpus mais parrudo, que contivesse letras de música de outros grupos de RAP, fazendo com que fosse necessário desviar da temática central, Racionais MC's. Seguem algumas informações visuais sobre os resultados do modelo.

### Testes de geração de texto
"""

from transformers import AutoTokenizer, AutoModelWithLMHead
import torch

tokenizer = AutoTokenizer.from_pretrained("pierreguillou/gpt2-small-portuguese")
model = AutoModelWithLMHead.from_pretrained("pierreguillou/gpt2-small-portuguese")

model.eval()

def format_output(output_tokens, tokenizer, tokens_per_line=8):
    decoded_text = tokenizer.decode(output_tokens.tolist(), skip_special_tokens=True)
    tokens = decoded_text.split()
    lines = [' '.join(tokens[i:i+tokens_per_line]) for i in range(0, len(tokens), tokens_per_line)]
    return '\n'.join(lines)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
text = "Tem que ser, tem que pá "
inputs = tokenizer(text, return_tensors="pt").to(device)

sample_outputs = model.generate(
    inputs.input_ids,
    pad_token_id=50256,
    do_sample=True,
    max_length=50,
    top_k=40,
    num_return_sequences=2
)

for i, sample_output in enumerate(sample_outputs):
    formatted_output = format_output(sample_output, tokenizer)
    print(">> Generated text {}\n\n{}".format(i+1, formatted_output))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
text = "Cotidiano na periferia"
inputs = tokenizer(text, return_tensors="pt").to(device)

sample_outputs = model.generate(
    inputs.input_ids,
    pad_token_id=50256,
    do_sample=True,
    max_length=50,
    top_k=40,
    num_return_sequences=2
)

for i, sample_output in enumerate(sample_outputs):
    formatted_output = format_output(sample_output, tokenizer)
    print(">> Generated text {}\n\n{}".format(i+1, formatted_output))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
text = "A lua cheia, clareia as ruas do Capão"
inputs = tokenizer(text, return_tensors="pt").to(device)

sample_outputs = model.generate(
    inputs.input_ids,
    pad_token_id=50256,
    do_sample=True,
    max_length=50,
    top_k=40,
    num_return_sequences=2
)

for i, sample_output in enumerate(sample_outputs):
    formatted_output = format_output(sample_output, tokenizer)
    print(">> Generated text {}\n\n{}".format(i+1, formatted_output))

"""# Base de dados

Importando bibliotecas que vão ser úteis neste estudo.
"""

import requests
from bs4 import BeautifulSoup
import re
import gc

"""## Baixando a base de dados"""

# URL do arquivo RAW
url_arquivo = 'https://raw.githubusercontent.com/JotaChina/Computer-intelligence/update%231/TP2/RacionaisLyrics.raw'

# Fazer solicitação HTTP para obter o conteúdo do arquivo
response = requests.get(url_arquivo)
if response.status_code == 200:
    # Extrair o texto do conteúdo da resposta
    texto_raw = response.text
else:
    print("Falha ao fazer solicitação HTTP")

"""## Pré-processando a base de dados.


*   Deixar o texto em minúsculo;
*   Remover caracteres não alfanuméricos.


"""

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\sáéíóúàèìòùâêîôûãõç]', '', text)

    # Dividir o texto em linhas
    lines = text.split('\n')

    # Remover linhas completamente em branco
    lines = [line for line in lines if line.strip()]

    # Juntar as linhas de volta em um único texto
    preprocessed_text = '\n'.join(lines)

    return preprocessed_text

"""Salvando o texto pré-processado em um arquivo txt e tornando-o em uma única string para manipular as sequências de entrada."""

texto = preprocess_text(texto_raw)
palavras = texto.split()
corpus = " ".join(palavras)

with open('letras.txt', 'w', encoding='utf-8') as f:
    f.write(texto)

print("Corpus salvo em letras.txt")

"""Utilizando o tokenizador do Keras (TensorFlow) para tokenizar o vocabulário."""

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts([corpus])
total_words = len(tokenizer.word_index) + 1
print(total_words)

"""Criando as sequências de entrada"""

import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf

# Converter o corpus em sequências de tokens
token_list = tokenizer.texts_to_sequences([corpus])[0]

# Criar input_sequences a partir do token_list
input_sequences = []
for i in range(1, int(len(token_list)*0.8)): #80% do corpus porque estava estourando a memória do colab
    n_gram_sequence = token_list[:i+1]
    input_sequences.append(n_gram_sequence)

# Pad sequences
max_sequence_len = 8  # Máximo de 8 tokens por linha
input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))

# Salvar n-grams em um arquivo de texto
with open('input_sequences.txt', 'w') as f:
    for seq in input_sequences:
        f.write(' '.join(map(str, seq)) + '\n')

# Liberar memória
del token_list
del input_sequences
del n_gram_sequence
gc.collect()

"""Criação das n-grams e fazendo one-hot encoding para y.
X será cada linha do texto pré-processado, exceto sua última palavra. Por sua vez, o conjunto y terá as últimas palavras de cada linha do texto.
"""

import numpy as np
from tensorflow.keras.utils import to_categorical

# Carregar n-grams de um arquivo de texto
loaded_sequences = []
with open('input_sequences.txt', 'r') as f:
    for line in f:
        loaded_sequences.append(list(map(int, line.strip().split())))

# Converter para array numpy
input_sequences = np.array(loaded_sequences)

# Definir X e y
X = input_sequences[:, :-1]
y = input_sequences[:, -1]

# Converter y para one-hot encoding
y = to_categorical(y, num_classes=total_words)

print("Shape de X:", X.shape)
print("Shape de y:", y.shape)

"""Fazendo a importação das bibliotecas necessárias para a manipulação da base de dados

# LSTM

Importando as bibliotecas para modelar e treinar a LSTM
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

"""## Criação da rede

Esta rede é composta de 4 camadas, onde a primeira delas é a camada de embedding, responsável por transformar os números inteiros (representando palavras) em vetores densos de tamanho 250, onde a saída desta camada será um tensor tridimensional.
A segunda camada é LSTM e possui 200 neurônios e retorna sequências completas em vez de apenas a saída do último passo de tempo e também tem como saída um tensor tridimensional.
A terceira camada é uma outra camada LSTM, porém com 100 neurônios, esta camada retorna apenas a saída do último passo de tempo, resultando em um tensor bidimensional.
Por fim, a quarta camada é uma camada densa, possuindo a quantidade de neurônios relativa à quantidade de palavras únicas disponíveis no texto e usa a função de ativação softmax para calcular as probabilidades de cada palavra no vocabulário ser a próxima palavra na sequência gerada.
"""

model = Sequential()
model.add(Embedding(total_words, 250, input_length=max_sequence_len-1))
model.add(LSTM(200, return_sequences=True))
model.add(LSTM(100))
model.add(Dense(total_words, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

"""## Treinamento do modelo"""

history = model.fit(X, y, epochs=100, batch_size=128, validation_data=(X, y))

"""## Salvando o modelo"""

model.save('racionais_model.h5')
print("Modelo salvo com sucesso.")

"""## Gráficos sobre acurácia e perda"""

import matplotlib.pyplot as plt

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""## Identificando as rimas presentes no texto"""

import re
import nltk
from nltk.tokenize import word_tokenize

# Baixar os recursos necessários do NLTK (caso ainda não tenham sido baixados)
nltk.download('punkt')

# Define uma expressão regular para encontrar a última palavra de cada linha
padrao_ultima_palavra = re.compile(r'\b(\w+)\s*$')

# Dicionário para armazenar as rimas por palavra final de linha
rimas_encontradas = {}

# Função para obter os 3 últimos caracteres de uma palavra
def obter_ultimos_tres_caracteres(palavra):
    return palavra[-3:]

# Ler o texto do arquivo letras.txt
with open('letras.txt', 'r', encoding='utf-8') as file:
    corpus = file.read()

# Dividir o corpus em linhas
linhas = corpus.split('\n')

# Para cada linha no corpus, encontra a última palavra e tokeniza as rimas
for linha in linhas:
    match = padrao_ultima_palavra.search(linha.lower())
    if match:
        ultima_palavra = match.group(1)
        # Obter os 3 últimos caracteres da última palavra
        ultimos_tres_caracteres = obter_ultimos_tres_caracteres(ultima_palavra)
        # Adiciona a rima tokenizada ao dicionário de rimas usando a lógica dos 3 últimos caracteres
        if ultimos_tres_caracteres in rimas_encontradas:
            rimas_encontradas[ultimos_tres_caracteres].add(ultima_palavra.lower())
        else:
            rimas_encontradas[ultimos_tres_caracteres] = {ultima_palavra.lower()}

# Converter os conjuntos de rimas de volta para listas (se necessário)
for rima, palavras in rimas_encontradas.items():
    rimas_encontradas[rima] = list(palavras)

# Exibir as rimas encontradas
print("Rimas encontradas:")
print(rimas_encontradas)

"""Biblioteca utilizada para corrigir palavras que possam estar incorretas na geração"""

!pip install pyspellchecker

"""## Função para gerar novas letras de RAP"""

import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences
from spellchecker import SpellChecker

# Função para gerar texto rimando com base nas rimas encontradas
def generate_text(seed_text, next_words, model, max_sequence_len, grupo_rimas):
    spell_checker = SpellChecker(language='pt')
    generated_text = seed_text.strip()  # Inicializa o texto gerado com o seed_text e remove espaços extras
    words_generated = 0  # Contador para controlar o número de palavras geradas
    last_word = ""  # Variável para armazenar a última palavra gerada em cada iteração
    last_line_last_word = ""  # Variável para armazenar a última palavra da linha anterior
    line_count = 0  # Contador de linhas geradas

    while words_generated < next_words:
        if words_generated == 0:
            generated_text += "\n"

        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')

        # Predicting probabilities for the next word
        predicted_probs = model.predict(token_list, verbose=0)[0]

        # Sampling the word index with highest probability
        predicted_index = np.argmax(predicted_probs)

        # Finding the corresponding word
        output_word = ""
        for word, index in tokenizer.word_index.items():
            if index == predicted_index:
                output_word = word
                break

        # Encontra a palavra corrigida se estiver incorreta
        corrected_word = spell_checker.correction(output_word)

        # Adiciona a palavra corrigida ao texto gerado e à semente
        seed_text += " " + corrected_word
        generated_text += " " + corrected_word
        words_generated += 1

        # Atualiza a última palavra gerada
        last_word = output_word

        # Se atingir o limite de palavras por linha, quebra a linha e atualiza a contagem de linhas
        if words_generated % 8 == 0:
            generated_text += "\n"
            line_count += 1

            # Verifica se é a primeira iteração para gerar apenas três linhas
            if words_generated == 8:
                line_count = 2

            # Pular uma linha a cada quatro linhas geradas
            if line_count % 4 == 0:
                generated_text += "\n"

            # Encontra a última palavra da linha atual
            last_line_words = generated_text.splitlines()[-1].split()
            last_line_last_word = last_line_words[-1].lower() if last_line_words else ""

            # Encontra a última palavra da linha anterior
            previous_line_words = generated_text.splitlines()[-2].split() if len(generated_text.splitlines()) > 1 else []
            previous_line_last_word = previous_line_words[-1].lower() if previous_line_words else ""

            # Verifica se há rimas disponíveis para a última palavra da linha anterior
            if previous_line_last_word in grupo_rimas:
                available_rhymes = [rhyme for rhyme in grupo_rimas[previous_line_last_word] if rhyme.endswith(last_line_last_word)]
                if available_rhymes:
                    rhyme_word = np.random.choice(available_rhymes)
                    generated_text += " " + rhyme_word
                    seed_text += " " + rhyme_word
                else:
                    generated_text += " " + output_word
                    seed_text += " " + output_word
            else:
                generated_text += " " + output_word
                seed_text += " " + output_word

    return generated_text

"""# Exemplos de utilização"""

generated_text = generate_text("A lua cheia, clareia as ruas do capão", 204, model, max_sequence_len-1, rimas_encontradas)
print(generated_text)

generated_text = generate_text("Tem que ser, tem que pa", 204, model, max_sequence_len-1, rimas_encontradas)
print(generated_text)

generated_text = generate_text("Cotidiano na periferia e as dificuldades", 204, model, max_sequence_len-1, rimas_encontradas)
print(generated_text)

generated_text = generate_text("A vida é diferente, da ponte pra cá", 204, model, max_sequence_len-1, rimas_encontradas)
print(generated_text)

"""# Conclusões

Os resultados iniciais são animadores e trazem ânsia de aplicar uma nova abordagem, talvez utilizar ambientes pagos para fazer a execução do treinamento e tentar inferir com um modelo mais fino. A inserção de modelos pré-treinados supracitados neste estudo é vista com entusiasmo para aproveitar o ambiente mais parrudo e apropriado para fazer as simulações. Sendo assim, outras técnicas de ajuste fino e regularização do modelo podem ser aplicadas, em busca de melhorar a acurácia das predições e, consequentemente, diminuir a perda durante as épocas de treinamento.

O desenvolvimento de uma rede neural para geração de novas letras de RAP é um desafio que traz a tona a correlação entre tecnologia e arte, em busca de aproximar-se do pensamento humano e suas formas de expressão. Além de ser uma homenagem a um dos maiores grupos de RAP da história, que são referências para tantas pessoas.

# Referências

* Satija, V. (2017). Rhymenet. https://github.com/vidursatija/rhymenet.
[Acesso em 21 jun. 2024].
* Souza, F., Nogueira, R., and Lotufo, R. (2020). BERTimbau: pretrained BERT models for Brazilian Portuguese. In 9th Brazilian Conference on Intelligent Systems, BRACIS, Rio Grande do Sul, Brazil, October 20-23 (to appear). [Acesso em 28 jun. 2024].
* Anand, A., Anand, A., Maiyuran, J., and Shum, M. Rap lyric generation: A phoneme-based lstm approach. [Acesso em 21 jun. 2024].
* Barrat, R. (2018). Rapping-neural-network. https://github.com/
robbiebarrat/rapping-neural-network. [Acesso em 21 jun. 2024].
* Draca, N. (2017). rddt. https://github.com/nikodraca/rddt. [Acesso em
21 jun. 2024].
* Guillou, P. (2020). Gportuguese-2 (portuguese gpt-2 small): a language model for portuguese text generation (and more nlp tasks...). [Acesso em 28 jun. 2024].
* JotaChina. Computer intelligence. https://github.com/JotaChina/
Computer-intelligence. [Acesso em 21 jun. 2024].
* Juliani, J. d. S. (2019). Gerando letras musicais utilizado uma rede neural recorrente lstm-long short-term memory. [Acesso em 21 jun. 2024].
* Leme, L. (2024). Finbert-pt-br. https://huggingface.co/lucas-leme/ FinBERT-PT-BR. [Acesso em: 25 jun. 2024].
* Letras.mus.br. Racionais mc’s. https://www.letras.mus.br/racionais-mcs/. [Acesso em 21 jun. 2024].
* Potash, P., Romanov, A., and Rumshisky, A. (2015). Ghostwriter: Using an lstm for automatic rap lyric generation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1919–1924. [Acesso em 21 jun. 2024].
"""