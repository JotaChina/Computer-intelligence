{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussão sucinta da Formalização do Problema e dos objetivos pretendidos\n",
    "\n",
    "Racionais MCs transcendem gerações e representam uma multiplicidade de experiências pessoais e contextos para inúmeras pessoas. Há até mesmo uma famosa frase que diz \"para toda situação da vida, um verso dos Racionais\". A motivação primordial, além de prestar homenagem a um dos maiores grupos de RAP da história, é a busca por novas linhas que se encaixem no contexto do grupo. Ao obter rimas coesas e uma progressão musical embasada, o objetivo é iniciar uma tentativa de generalização para o contexto do RAP. Os algoritmos de treinamento desempenham um papel fundamental para o sucesso desse empreendimento, requerendo a seleção cuidadosa dos métodos de classificação e o treinamento da base de dados. Assim, os principais objetivos incluem a classificação das rimas para garantir uma distribuição coesa e significativa do ponto de vista lírico. Isso será alcançado através do treinamento com a base de dados BERTimbau, adaptada ao universo dos Racionais MCs, de modo a incorporar o contexto previamente estabelecido. O resultado esperado é a geração de novas letras de músicas que ressoem com a essência e a autenticidade do grupo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Análise Exploratória dos Dados, detalhando cada etapa realizada, justificando as escolhas e justificando as transformações efetuadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A base de dados é adequada?\n",
    "Inicialmente a ideia de utilizar letras do Racionais foi boa, pois os textos são grandes e dificilmente repetem-se. Porém, a necessidade do aprendizado do contexto das letras é um pouco mais complicado de ser compreendido pelo algoritmo. \n",
    "### Existem dados faltantes? Como foram tratados?\n",
    "Nesta segunda etapa, a intenção é tentar generalizar o contexto, inserindo mais letras do Racionais e, também, algumas participações do grupo ou dos artistas individualmente para que possa ser entendido o universo de interesse. \n",
    "### Existem variáveis categóricas?\n",
    "Sim, as rimas. As transformações necessárias envolvem a predição de novas letras a partir das rimas que são sorteadas. Sendo assim, as rimas necessitam ser ordenadas em pares para que possam ser sorteadas. \n",
    "### Principais Transformações\n",
    "A geração de novas letras se torna mais fácil a partir da utilização da base BERTimbau, pois já há uma geração de texto e predição de palavras. A maior dificuldade vem na criação de novas letras com uma qualidade lírica mais apurada, a análise das features e tratamento dos dados pode ser uma boa solução, portanto tentei com .... \n",
    "### Informações estatísticas e Visualização de algumas das features principais\n",
    "Plotar os gráficos de rimas, pares de rimas iguais, quantidade de rimas com algumas sílabas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quais foram as principais decisões de projeto nessa etapa? \n",
    "\n",
    "Ao longo desta discussão, algumas das principais decisões de trabalho foram:\n",
    "\n",
    "* Escolha do modelo pré-treinado: Optou-se por utilizar o modelo BERT pré-treinado em português (bert-base-portuguese-cased ou BERTimbau) fornecido pela comunidade do Hugging Face. Essa escolha foi baseada na disponibilidade do modelo e na sua capacidade de lidar com tarefas de processamento de linguagem natural em português.\n",
    "* Pré-processamento do texto: Decidiu-se realizar etapas de pré-processamento no texto antes de tokenizá-lo. Isso incluiu remover caracteres especiais, pontuação e números, bem como converter todo o texto em minúsculas. Essas etapas ajudaram a limpar o texto e a padronizá-lo para o tokenizador.\n",
    "* Tokenização: Utilizou-se o tokenizador BERT para tokenizar o texto pré-processado. Foi definido um comprimento máximo de tokens para garantir que o texto tokenizado pudesse ser processado eficientemente pelo modelo.\n",
    "* Treinamento do modelo: Treinou-se o modelo BERT usando o texto tokenizado. Isso envolveu o uso de um otimizador para ajustar os pesos do modelo com base nas previsões e no cálculo da perda em cada época de treinamento.\n",
    "* Avaliação da acurácia: Durante o treinamento, implementou-se uma métrica para avaliar a acurácia das previsões do modelo. Isso permitiu monitorar o desempenho do modelo ao longo do tempo e identificar áreas de melhoria.\n",
    "* Exploração dos resultados: Após o treinamento, explorou-se os resultados, incluindo a distribuição de tokens, as palavras mais frequentes e as rimas mais comuns. Isso forneceu insights sobre o conteúdo e a estrutura do texto e nos ajudou a entender melhor o comportamento do modelo.\n",
    "* Inserir novas entradas na base de dados, pois foi perceptível a necessidade de mais informações para treinar o modelo;\n",
    "* Quais os métodos que serão utilizados (?)\n",
    "* \n",
    "\n",
    "## Revisão da Literatura: discuta artigos e projetos semelhantes que você se baseou para suas escolhas.\n",
    "Estes trabalhos trazem a temática de geração novas letras de música a partir de algoritmos de deeplearning. Porém, abordam de maneiras diferentes e acrescem no aprendizado nesta temática, pois é possível observar propostas diferentes e compreender os algoritmos e suas utilizações nestes casos. Portanto, será discorrido um pouco mais sobre algumas das referências utilizadas para este trabalho.\n",
    "\n",
    "## Como pretende avaliar o impacto dessas decisões na etapa seguinte deste trabalho?\n",
    "A partir dos testes de predição com o modelo pre treinado com letras do Racionais, em caso de acurácia alta, será implementada a classificação das rimas para que comecem a ser geradas novas letras de RAP. \n",
    "As rimas serão classificadas .... \n",
    "Portanto, o impacto destas decisões vão refletir em um texto mais coeso, sendo assim ......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descrever, sucintamente, a metodologia para execução das simulações computacionais da etapa seguinte. \n",
    "As simulações terão como objetivo central a geração de novas linhas de RAP que rimem, façam sentido líricamente e sejam coesas em sua temática central. A princípio, as rimas serão separadas em:\n",
    "* Pares de rimas, na ordem em que acontecem, para que a geração de novas linhas tenham as \"mesmas\" rimas que em letras originais;\n",
    "* Rimas iguais, ou com o mesmo fim de palavra, para que as rimas possam ser facilmente encaixadas e o custo maior seja para manter a coesão textual; \n",
    "* Rimas na metade e nos finais dos versos, melhorando a métrica e a lírica, porém, será mais custoso de implementar;\n",
    "* Dividir a base de dados entre treinamento e teste, pode ser que aumente a acurácia final do algoritmo;\n",
    "## Qual(is) Algoritmo(s) de Machine/Deep Learning pretende utilizar?\n",
    "Durante o processo discutido, focou-se principalmente no uso de modelos de linguagem pré-treinados, especificamente o modelo BERT (Bidirectional Encoder Representations from Transformers). O BERT é uma arquitetura de rede neural baseada em transformers e é treinado em grandes corpora de texto não supervisionado. Ele captura a bidirecionalidade do contexto em uma sequência de palavras, permitindo uma compreensão mais profunda e contextualizada do texto.\n",
    "Embora o BERT seja frequentemente usado em tarefas de NLP (Processamento de Linguagem Natural), como classificação de texto, marcação de sequência e preenchimento de lacunas, neste caso, está sendo utilizado para gerar novas letras de músicas com a temática do Racionais. Isso foi feito treinando o modelo BERT em um conjunto de dados composto por letras de músicas pré-processadas e tokenizadas do Racionais.\n",
    "Além do BERT, outros algoritmos de machine learning não foram diretamente utilizados neste processo. No entanto, para treinar o modelo BERT, foram empregadas técnicas de otimização como AdamW (uma variação do algoritmo de otimização Adam) e calculamos a perda (loss) durante o treinamento para atualizar os pesos do modelo.\n",
    "\n",
    "Além do BERT, existem outros algoritmos de machine learning que podem ser explorados para tarefas relacionadas ao processamento de linguagem natural (NLP) e geração de texto. Aqui estão alguns exemplos:\n",
    "\n",
    "1. **GPT (Generative Pre-trained Transformer)**: Similar ao BERT, o GPT é uma arquitetura de modelo de linguagem baseada em transformers, mas foca na geração de texto. Ele é treinado de forma não supervisionada em grandes corpora de texto e é capaz de gerar texto coerente e contextualmente relevante.\n",
    "\n",
    "2. **LSTM (Long Short-Term Memory)**: LSTM é uma arquitetura de rede neural recorrente (RNN) projetada para lidar com sequências de dados, como texto. É conhecido por capturar dependências de longo prazo em dados sequenciais, o que o torna útil para tarefas de geração de texto, tradução automática, entre outras.\n",
    "\n",
    "3. **Transformer-XL**: Outra variação da arquitetura transformer, o Transformer-XL é projetado para capturar dependências de longo prazo em sequências de texto. Ele é particularmente útil para modelar textos mais longos, mantendo uma memória mais longa de contextos anteriores.\n",
    "\n",
    "4. **Seq2Seq (Sequence-to-Sequence)**: Este é um framework que consiste em dois modelos RNN, um codificador e um decodificador, usado principalmente para tarefas de tradução automática e geração de respostas em chatbots. Ele é capaz de traduzir uma sequência de entrada em uma sequência de saída.\n",
    "\n",
    "5. **Word2Vec e GloVe**: Embora não sejam modelos de geração de texto, Word2Vec e GloVe são algoritmos populares de embedding de palavras que mapeiam palavras em vetores de números reais. Esses vetores de palavras podem ser usados como entrada para outros modelos de machine learning ou deep learning em tarefas de processamento de linguagem natural.\n",
    "\n",
    "## Qual(is) métrica(s) de desempenho pretende utilizar?\n",
    "\n",
    "Para avaliar o desempenho de modelos de processamento de linguagem natural (NLP) e geração de texto, serão testadas várias métricas.\n",
    "\n",
    "1. **Perplexidade**: É uma métrica comum para modelos de linguagem que mede quão bem o modelo é capaz de prever uma sequência de palavras. Quanto menor a perplexidade, melhor o desempenho do modelo.\n",
    "\n",
    "2. **BLEU (Bilingual Evaluation Understudy)**: É uma métrica de avaliação automática usada para avaliar a qualidade de traduções geradas por sistemas de tradução automática. Ela compara a saída do sistema com uma ou mais traduções de referência.\n",
    "\n",
    "3. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: É uma família de métricas usadas para avaliar a qualidade de resumos automáticos e geração de texto. Ele compara a similaridade entre o texto gerado e um ou mais textos de referência, com base em métricas de sobreposição de palavras.\n",
    "\n",
    "4. **Accuracy (Precisão)**: Para tarefas de classificação de texto, como análise de sentimento, classificação de tópicos, entre outras, a precisão pode ser uma métrica relevante que mede a proporção de previsões corretas em relação ao total de previsões.\n",
    "\n",
    "5. **F1-Score**: É uma métrica que combina precisão e recall em uma única pontuação, calculada como a média harmônica entre os dois. É útil em tarefas de classificação binária e multiclasse.\n",
    "\n",
    "6. **Perda (Loss)**: Em treinamentos de modelos de aprendizado supervisionado, a perda é frequentemente usada como uma métrica para avaliar a eficácia do modelo durante o treinamento. O objetivo é minimizar a perda ao longo do tempo.\n",
    "\n",
    "## Pretende comparar seus resultados com qual(is) referência(s) (outros modelos; trabalhos relacionados)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
